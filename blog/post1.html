<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Discuss : What Matters in Transformers? Not All Attention Is Needed</title>
    <link rel="stylesheet" href="./blog-css/post1_css.css">
</head>
<body>
    <header>
        <h1>Discuss: What Matters in Transformers? Not All Attention Is Needed</h1>
    </header>
    <main>
        <section>
            <h2>Introduction</h2>
            <p>
                The famous work <strong>ATTENTION IS ALL YOU NEED</strong> has been the basis of a lot of Transformer-based LLM progress and AI research. Transformer-based LLMs have presented amazing capabilities in multiple tasks. Scaling these models has demonstrated performance along with <i>inflated deployment costs</i> and <i>compute resource demands</i>. Techniques such as KV-cache, Quantization, Pruning, LoRA, MoRA, and others have evolved as a result of research to bring efficiency in memory and computation for these LLMs.
            </p>
            <p>
                The paper <strong>WHAT MATTERS IN TRANSFORMERS? NOT ALL ATTENTION IS NEEDED</strong> , shares their <i>investigation on redundancy</i> in different modules of Transformer - <i>MLP layers, Attention layers and Block modules</i>. Also it proposes <i>Attention Drop</i> and <i>Joint Layer Drop</i> methods to increase the model speedup without compromising its original performance.
            </p>
            <p>
                A transformer block consists of both an Attention layer and an MLP layer. The Attention layer facilitate contextual information flow between tokens. The MLP layer transforms the token representations. As the layers have different roles, redundancy in the two layers is assessed separately, and also at Block level.
            </p>
        </section>
        <section>
            <h2>Redundancy & Drop Assessment</h2>
            <p>
                Block Drop and Layer Drop, i.e, pruning redundant Block and pruning redundant layers. [Iteratively compute the importance score for each module(block or layer) from shallow to deep, and drop modules with lowest scores].
            </p>
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig1_1.png" alt="Block Drop Visualization" style="max-width: 100%; border-radius: 8px;">
                            
                        </figure>
                    </td>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig1_2.png" alt="Layer Drop Visualization" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 1: Fire sign represents block & layer with high similarity.</i> <br>
                <i>Blurred parts represent dropped Blocks & Layers.</i>
            </span>
            <p>
                <strong>Block Drop</strong> degrades the model's original performance significantly with increased speedup. This is because it overlooks the internal fine-grained architectures within each block.
                <strong>MLP Layer Drop</strong> also gives huge performance degradation with moderate speedup. This is because of little redundancy(more importance) in MLP layer modules. 
                But <i>in attention layers, redundancy is significant, particularly in deeper layers, and is consistent across training stages.</i> 
                So, <strong>Attention Layer Drop</strong> gives minimal impact on original performance with high speedup. 
                Thus, the proposed methods in the paper, which brings <strong>memory efficiency and speedup, without compromising much on performance</strong>, mainly focus on Attention Layers redundancy.
            </p>
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig2_1.png" alt="Importance of Blocks" style="max-width: 100%; border-radius: 8px;">
                            
                        </figure>
                    </td>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig2_2.png" alt="Importance of Layers" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 2: Importance scores of modules from shallow to deep.</i> <br>
                <i>Lighter the color, less the importance score, which means deeper modules are redundant.</i>
            </span>
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig3.png" alt="Visualization of Importance Scores" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 3: Redundancy(light yellow) in attention layers is consistent across training stages.</i>
            </span>
            <p>
                Additionally, Block Drop and Layer Drop are <strong><i>orthogonal to quantization</i></strong>, and their integration with quantization significantly <i>enhances efficiency</i>.
            </p>
        </section>
        <section>
            <h2>Methods</h2>
            <h3>Attention Drop and Joint Layer Drop</h3>
            <p>
                Both the methods use a <i>Similarity Metric</i> to identify redundancy, and do <i>Structured Pruning</i> i.e, dropping redundant modules. 
                It is actually <i>One Shot Dropping</i> in a <i>Training Free manner</i>. 
                In other words, it is a <i>Post-training dropping</i> without involving retraining.
            </p>
            <p>
                SIMILARITY METRIC - The important modules are expected to significantly alter their input, while the redundant modules produce outputs similar to inputs. 
                This is the basis of computing redundancy. The <i>Cosine Similarity between the hidden state of input X and hidden state of output Y of the module, is the measure of redundancy.</i> 
                And the <strong><i>importance score</i></strong> S is computed as: <strong>`S = 1 − CosineSim(X,Y)`</strong>
            </p>
            <p>
                <strong>ATTENTION DROP -</strong><br>
                1. Calculate importance score for each Attention Layer module.<br>
                2. Define a pruning or dropping ratio. [0-100%]<br>
                3. Prune Attention Layers with least importance scores. (according to the Dropping Ratio)
            </p>
            <p>
                <strong>JOINT LAYER DROP -</strong><br>
                1. Calculate the importance scores for both attention layers(SA) and MLP layers(SM) individually. <br>
                2. Define a pruning or dropping ratio. [0-100%] <br>
                3. Concatenate the scores into a single array: S = [SA, SM] <br>
                4. From the combined set of importance scores, drop the layers with the lowest values, regardless of whether they are attention or MLP layers. (according to the pruning ratio)
            </p>
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig4.png" alt="Accuracy Curves of Dropping different Target Modules" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 4: Joint Layer Drop constantly achieves better performance than others.</i>
            </span>
            <p>
                <strong>Joint Layer Drop consistently achieves better performance than either Attention Drop or MLP Drop alone.</strong> 
                It begins by exclusively dropping attention layers, which are typically more redundant than MLP layers. As a result, in the initial stages of pruning, the performance of Joint Layer Drop overlaps with that of Attention Drop. However, as the dropping ratio increases and the more redundant attention layers are pruned, MLP layers start to become the next most redundant components. At this point, Joint Layer Drop begins to remove MLP layers, leading to further reductions in redundant layers without significant performance loss, 
                <i>e.g., after dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90% of the performance on the MMLU task.</i>
            </p>
            
        </section>
        <section>
            <h2>Evaluating Metrics for Methods</h2>
            <p>
                <code>SPEEDUP DEGRADATION RATIO : SDR = ∆Avg. / ∆Speedup</code>, <br>
                <code> ∆Avg. = the % change in average performance across the evaluated tasks, </code><br> 
                <code> ∆Speedup = the % of speedup achieved. </code><br>
                SDR measures the amount of performance degradation incurred for each 1% increase in speedup.
            </p>
        </section>
        <section>
            <h2>POSITIVE EXAMPLES</h2>            
            <ul>
                <li>Llama-2-13B - 99% original performance, after dropping 8 attn. Layers.</li>
                <li>Mistral-7B - 99% original performance, after dropping 8 attn. Layers.</li>
                <li>Llama-2-13B - 90% original performance(MMLU task), dropped 31 layers (Attn+MLP).</li>
                <li>Llama-2-70B - 97.6% original performance. 48.4% speedup.</li>
            </ul>
        </section>
        <section>
            <h2>NEGATIVE EXAMPLES</h2>
            <p>
                <strong>Llama-2-7B-Math</strong> model’s ability <i>rapidly deteriorates</i>, when Attention Layers are dropped.
                <i>Reason</i> : Instruction finetuning of Llama-2-7B-Base(originally poor in mathematics), created Llama-2-7B-Math with math ability. This ability obtained solely through FT appears to be <i>superficial</i>.
            </p>
        </section>
        <section>
            <h2>Open Questions</h2>
            <ul>
                <li>Measure of redundancy - is it valid for different transformer architectures ?</li>
                <li>High sparsity conditions - is it related only to model weights precisely zero ?</li>
                <li>Decision of one-shot vs iterative is taken over tasks accuracy - it could be different for other architectures ?</li>
                <li>Redundancy is an inherent property of Transformers architecture - further optimized architecture.</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Akash Goyal | Blog on AI/ML</p>
    </footer>
</body>
</html>
