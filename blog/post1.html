<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Discuss : What Matters in Transformers? Not All Attention Is Needed</title>
    <link rel="stylesheet" href="./blog-css/post1_css.css">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <h1>Discuss: What Matters in Transformers? Not All Attention Is Needed</h1>
    </header>
    <main>
        <section style="background-color: #e7f3fe; border: 1px solid #b3d8ff; padding: 15px; border-radius: 5px;">
            <p>
                <h3 style="color: #0074cc;">AS YOU READ, RELATE WITH BELOW QUESTIONS. IT WOULD BE FUN ...</h3>
                <ul style="list-style-type: none; padding: 0;">
                    <li style="margin-bottom: 10px;">1. Why deep attention layers have so much redundancy?</li>
                    <li style="margin-bottom: 10px;">2. Why joint-drop works better in high sparsity cases?</li>
                    <li style="margin-bottom: 10px;">3. Why quantization enhance efficiency of these methods?</li>
                    <li style="margin-bottom: 10px;">4. Why joint-drop excel attention-drop? (an easy one)</li>
                    <li style="margin-bottom: 10px;">5. Why one-shot drop excelled iterative drop?</li>
                </ul>
            </p>
        </section>
        
        <section>
            <h2>Introduction</h2>
            <p>
                The famous work <strong>ATTENTION IS ALL YOU NEED</strong> has been the basis of a lot of Transformer-based LLM progress and AI research. Transformer-based LLMs have presented amazing capabilities in multiple tasks. Scaling these models has demonstrated performance along with <i>inflated deployment costs</i> and <i>compute resource demands</i>.
            </p>
            <p>
                The paper <strong>WHAT MATTERS IN TRANSFORMERS? NOT ALL ATTENTION IS NEEDED</strong> , shares their <i>investigation on redundancy</i> in different modules of Transformer - <i>MLP layers, Attention layers and Block modules</i>. Also it proposes <i>Attention Drop</i> and <i>Joint Layer Drop</i> methods to increase the model speedup without compromising its original performance.
                The Attention layer facilitate contextual information flow between tokens. The MLP layer transforms the token representations.
            </p>
        </section>
        <section>
            <!--h2>Visualize Dropping</h2-->
            <p>
                <i>From the below image, we can understand that a (block) or (Layer+LayerNorm) is fully removed</i><br> 
                <i>Then, the remaining (blocks) or (Layer Module), adjusts the respective skip-connections and input outputs links.</i>
            </p>
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig1_1.png" alt="Block Drop Visualization" style="max-width: 100%; border-radius: 8px;">
                            
                        </figure>
                    </td>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig1_2.png" alt="Layer Drop Visualization" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 1: Fire sign represents block & layer with high similarity.</i> <br>
                <i>Blurred parts represent dropped Blocks & Layers.</i>
            </span>
        </section>
        <section>
            <!--h2>Drop Assessment</h2-->
            <p>
                <strong>Block Drop</strong> degrades the model's original performance significantly with increased speedup. This is because it overlooks the internal fine-grained architectures within each block.<br>
                <strong>MLP Layer Drop</strong> also gives huge performance degradation with moderate speedup. This is because of little redundancy(more importance) in MLP layer modules. <br>
                But <i>in attention layers, redundancy is significant, particularly in deeper layers, and is consistent across training stages.</i> 
                So, <strong>Attention Layer Drop</strong> gives minimal impact on original performance with high speedup. <br>
                <!--Thus, the proposed methods in the paper, which brings <strong>memory efficiency and speedup, without compromising much on performance</strong>, mainly focus on Attention Layers redundancy.-->
                <i>By looking at range of light yellow color(less importance score), it is obvious that attention layer have most redundancy.</i>
            </p>
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig2_1.png" alt="Importance of Blocks" style="max-width: 100%; border-radius: 8px;">
                            
                        </figure>
                    </td>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig2_2.png" alt="Importance of Layers" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 2: Importance scores of modules from shallow to deep.</i> <br>
                <i>Lighter the color, less the importance score, which means deeper modules are redundant.</i>
            </span>
            <p>
                Block Drop and Layer Drop's integration with <strong>quantization</strong> significantly <i>enhances efficiency</i>. 
                This happens possibly because of increase in sparsity after quantization(fp16 mentioned in paper).
            </p>
        </section>
        <section>
            <h2>Methods</h2>
            <h4 style="text-align: center;">[Similarity Metric, One Shot Dropping, Training Free manner, Post-training dropping]</h4>
            <p>
                CORE IDEA : The important modules are expected to significantly alter their input, while the redundant modules produce outputs similar to inputs. 
                <i>Thus, modules with very high cosine similarity(X,Y) are redundant modules.</i><br>
                The <strong><i>importance score</i></strong> S is computed as: <strong> S = 1 âˆ’ CosineSim(X,Y) </strong> <br>
                where,
                <i>CosineSim(X,Y) is Cosine Similarity between the input X and output Y of the module.</i>

                <br><br>Read the algorithm of two proposed methods below : 

            </p>
            <div style="background-color: #ffffcc; padding: 20px; border: 1px solid #cccc00; border-radius: 5px; font-family: 'Poppins', sans-serif;">
                <table style="width: 100%; border-spacing: 20px; text-align: center; border: 1px solid black; font-size: 16px; line-height: 1.6;">
                    <tr>
                        <td style="border: 1px solid black; font-weight: 600;"><strong>ATTENTION DROP</strong></td>
                        <td style="border: 1px solid black; font-weight: 600;"><strong>JOINT LAYER DROP</strong></td>
                    </tr>
                    <!--tr>
                        <td colspan="2" style="padding: 0;">
                            <hr style="border: none; border-top: 1px solid black; margin: 0;">
                        </td>
                    </tr-->
                    <tr>
                        <td style="vertical-align: top; border-right: 1px solid black;">
                            <strong>1.</strong> Calculate importance score for each Attention Layer module.<br>
                            <strong>2.</strong> Define a dropping ratio(DR).[0-100%]<br>
                            <strong>3.</strong> Prune Attention Layers with least importance scores. (according to DR)
                        </td>
                        <td style="vertical-align: top;">
                            <strong>1.</strong> Calculate the importance scores for both attention layers(SA) and MLP layers(SM) individually.<br>
                            <strong>2.</strong> Concatenate the scores into a single array: S = [SA, SM]<br>
                            <strong>3.</strong> Define a pruning ratio(DR).[0-100%]<br>
                            <strong>4.</strong> From the combined set of importance scores, drop the layers with the lowest values, regardless of whether they are attention or MLP layers. (according to DR)
                        </td>
                    </tr>
                </table>
            </div>
            
            <p>
                <strong>Joint Layer Drop consistently achieves better performance than either Attention Drop or MLP Drop alone.</strong> <br>
                <strong>Explanation</strong> - For same dropping ratio(d%), 'joint-drop' removes least important layers from larger set.
                The 'individual layer-drop' may remove a meaningful layer(because of small set of layers) to fulfill the dropping ratio. 
                It can be validated from the curves in below image.
            </p>
    
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig4.png" alt="Accuracy Curves of Dropping different Target Modules" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 4: Joint Layer Drop constantly achieves better performance than others.</i>
            </span>
            
        </section>
        <section>
            <h4>Evaluating Metrics for Methods</h4>
            <p>
                <code>SPEEDUP DEGRADATION RATIO : SDR = âˆ†Avg. / âˆ†Speedup</code>, <br>
                <code> âˆ†Avg. = the % change in average performance across the evaluated tasks, </code><br> 
                <code> âˆ†Speedup = the % of speedup achieved. </code><br>
                SDR measures the amount of performance degradation incurred for each 1% increase in speedup.
            </p>
        </section>
        <section>
            <h2>POSITIVE EXAMPLES</h2>            
            <ul>
                <li>Llama-2-13B - 99% original performance, after dropping 8 attn. Layers.</li>
                <li>Llama-2-13B - 90% original performance(MMLU task), dropped 31 layers (Attn+MLP).</li>
                <li>Llama-2-70B - 97.6% original performance. 48.4% speedup.</li>
            </ul>
        </section>
        <section>
            <h2>NEGATIVE EXAMPLES</h2>
            <p>
                <strong>Llama-2-7B-Math</strong> modelâ€™s ability <i>rapidly deteriorates</i>, when Attention Layers are dropped.
                <i>Reason</i> : Instruction finetuning of Llama-2-7B-Base(originally poor in mathematics), created Llama-2-7B-Math with math ability. This ability obtained solely through FT appears to be <i>superficial</i>.
            </p>
        </section>
        <section style="background-color: #e7f3fe; border: 1px solid #b3d8ff; padding: 15px; border-radius: 5px;">
            <p>
                <h3 style="color: #0074cc;">Wanna think more ...</h3>
                <ul>
                    <li style="margin-bottom: 10px;">Measure of redundancy - it's validity for different transformer architectures ?</li>
                    <li style="margin-bottom: 10px;">High sparsity conditions - is it related only to model weights precisely zero ?</li>
                    <li style="margin-bottom: 10px;">Decision of one-shot vs iterative,(taken on basis of task) - any impact of architectures ?</li>
                </ul>
            </p>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Akash Goyal | Blog on AI/ML</p>
    </footer>
</body>
</html>
