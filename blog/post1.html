<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Discuss : What Matters in Transformers? Not All Attention Is Needed</title>
    <link rel="stylesheet" href="./blog-css/post1_css.css">
</head>
<body>
    <header>
        <h1>Discuss: What Matters in Transformers? Not All Attention Is Needed</h1>
    </header>
    <main>
        <section>
            <h2>Introduction</h2>
            <p>
                The famous work <strong>ATTENTION IS ALL YOU NEED</strong> has been the basis of a lot of Transformer-based LLM progress and AI research. Transformer-based LLMs have presented amazing capabilities in multiple tasks. Scaling these models has demonstrated performance along with <i>inflated deployment costs</i> and <i>compute resource demands</i>.
            </p>
            <p>
                The paper <strong>WHAT MATTERS IN TRANSFORMERS? NOT ALL ATTENTION IS NEEDED</strong> , shares their <i>investigation on redundancy</i> in different modules of Transformer - <i>MLP layers, Attention layers and Block modules</i>. Also it proposes <i>Attention Drop</i> and <i>Joint Layer Drop</i> methods to increase the model speedup without compromising its original performance.
                The Attention layer facilitate contextual information flow between tokens. The MLP layer transforms the token representations.
            </p>
        </section>
        <section>
            <h2>Visualize Dropping</h2>
            <p>
                <i>From the below image, we can understand that a (block) or (Layer+LayerNorm) is fully removed</i><br> 
                <i>Then, the remaining (blocks) or (Layer Module), adjusts the respective skip-connections and input outputs links.</i>
            </p>
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig1_1.png" alt="Block Drop Visualization" style="max-width: 100%; border-radius: 8px;">
                            
                        </figure>
                    </td>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig1_2.png" alt="Layer Drop Visualization" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 1: Fire sign represents block & layer with high similarity.</i> <br>
                <i>Blurred parts represent dropped Blocks & Layers.</i>
            </span>
        </section>
        <section>
            <h2>Drop Assessment</h2>
            <p>
                <strong>Block Drop</strong> degrades the model's original performance significantly with increased speedup. This is because it overlooks the internal fine-grained architectures within each block.<br>
                <strong>MLP Layer Drop</strong> also gives huge performance degradation with moderate speedup. This is because of little redundancy(more importance) in MLP layer modules. <br>
                But <i>in attention layers, redundancy is significant, particularly in deeper layers, and is consistent across training stages.</i> 
                So, <strong>Attention Layer Drop</strong> gives minimal impact on original performance with high speedup. <br>
                <!--Thus, the proposed methods in the paper, which brings <strong>memory efficiency and speedup, without compromising much on performance</strong>, mainly focus on Attention Layers redundancy.-->
                <i>By looking at range of light yellow color(less importance score), it is obvious that attention layer have most redundancy.</i>
            </p>
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig2_1.png" alt="Importance of Blocks" style="max-width: 100%; border-radius: 8px;">
                            
                        </figure>
                    </td>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig2_2.png" alt="Importance of Layers" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 2: Importance scores of modules from shallow to deep.</i> <br>
                <i>Lighter the color, less the importance score, which means deeper modules are redundant.</i>
            </span>
            <p>
                Block Drop and Layer Drop's integration with <strong>quantization</strong> significantly <i>enhances efficiency</i>. 
                This happens possibly because of increase in sparsity after quantization.
            </p>
        </section>
        <section>
            <h2>Methods</h2>
            <h4 style="text-align: center;">[Similarity Metric, One Shot Dropping, Training Free manner, Post-training dropping]</h4>
            <p>
                CORE IDEA : The important modules are expected to significantly alter their input, while the redundant modules produce outputs similar to inputs. 
                <i>Thus, modules with very high cosine similarity(X,Y) are redundant modules.</i><br>
                The <strong><i>importance score</i></strong> S is computed as: <strong> S = 1 − CosineSim(X,Y) </strong> <br>
                where,
                <i>CosineSim(X,Y) is Cosine Similarity between the input X and output Y of the module.</i>

                <br><br>Read the algorithm of two proposed methods below : 

            </p>
            <table style="width: 100%; border-spacing: 20px; text-align: center; border: 1px solid black; ">
                <tr>
                    <td style="border: 1px solid black;"><strong>ATTENTION DROP</strong></td>
                    <td style="border: 1px solid black;"><strong>JOINT LAYER DROP</strong></td>
                </tr>
                <tr>
                    <td style="vertical-align: top;">
                            <!--strong>ATTENTION DROP -</strong><br-->
                            1. Calculate importance score for each Attention Layer module.<br>
                            2. Define a pruning or dropping ratio. [0-100%]<br>
                            3. Prune Attention Layers with least importance scores. (according to the Dropping Ratio)
                    </td>
                    <td style="vertical-align: top;">
                            <!--strong>JOINT LAYER DROP -</strong><br-->
                            1. Calculate the importance scores for both attention layers(SA) and MLP layers(SM) individually. <br>
                            2. Define a pruning or dropping ratio. [0-100%] <br>
                            3. Concatenate the scores into a single array: S = [SA, SM] <br>
                            4. From the combined set of importance scores, drop the layers with the lowest values, regardless of whether they are attention or MLP layers. (according to the pruning ratio)

                    </td>
                </tr>
            </table>
            <p>
                It is clearly visible from the curves in below image, <strong>Joint Layer Drop consistently achieves better performance than either Attention Drop or MLP Drop alone.</strong> <br>
                <strong>Explanation</strong> - For same dropping ratio(d %), joint dropping removes least important layers from larger set.
                and individually dropping layers, may remove a meaningful layer(because of small set of layers) to fulfill the dropping ratio.
            </p>
    
            <table style="width: 100%; border-spacing: 20px; text-align: center;">
                <tr>
                    <td style="vertical-align: top;">
                        <figure>
                            <img src="./images/post1/fig4.png" alt="Accuracy Curves of Dropping different Target Modules" style="max-width: 100%; border-radius: 8px;">
                        </figure>
                    </td>
                </tr>
            </table>
            <span class="fig-caption">
                <i>Figure 4: Joint Layer Drop constantly achieves better performance than others.</i>
            </span>
            
        </section>
        <section>
            <h2>Evaluating Metrics for Methods</h2>
            <p>
                <code>SPEEDUP DEGRADATION RATIO : SDR = ∆Avg. / ∆Speedup</code>, <br>
                <code> ∆Avg. = the % change in average performance across the evaluated tasks, </code><br> 
                <code> ∆Speedup = the % of speedup achieved. </code><br>
                SDR measures the amount of performance degradation incurred for each 1% increase in speedup.
            </p>
        </section>
        <section>
            <h2>POSITIVE EXAMPLES</h2>            
            <ul>
                <li>Llama-2-13B - 99% original performance, after dropping 8 attn. Layers.</li>
                <li>Mistral-7B - 99% original performance, after dropping 8 attn. Layers.</li>
                <li>Llama-2-13B - 90% original performance(MMLU task), dropped 31 layers (Attn+MLP).</li>
                <li>Llama-2-70B - 97.6% original performance. 48.4% speedup.</li>
            </ul>
        </section>
        <section>
            <h2>NEGATIVE EXAMPLES</h2>
            <p>
                <strong>Llama-2-7B-Math</strong> model’s ability <i>rapidly deteriorates</i>, when Attention Layers are dropped.
                <i>Reason</i> : Instruction finetuning of Llama-2-7B-Base(originally poor in mathematics), created Llama-2-7B-Math with math ability. This ability obtained solely through FT appears to be <i>superficial</i>.
            </p>
        </section>
        <section>
            <h2>Open Questions</h2>
            <ul>
                <li>Measure of redundancy - is it valid for different transformer architectures ?</li>
                <li>High sparsity conditions - is it related only to model weights precisely zero ?</li>
                <li>Decision of one-shot vs iterative is taken over tasks accuracy - it could be different for other architectures ?</li>
                <li>Redundancy is an inherent property of Transformers architecture - further optimized architecture.</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Akash Goyal | Blog on AI/ML</p>
    </footer>
</body>
</html>
